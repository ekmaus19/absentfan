#
# Scrape the entries of a story
#
#---------------------------------------

#------------------------------------------------------------
# get to the index page of fandom
#------------------------------------------------------------
getUrlTitle <- function(story, type, title) {
  # look through typeMedia for href
  if(grepl("'", story)) {
    story<-str_split(story,"'")
    var <- "\\ \\'"
    var <- str_split(var, " ")
    story <- paste0(story[[1]][1], var[[1]][2], story[[1]][2])
  }
  
  rowVal <- grep(story,typeMedia[[type]]$title)
  href <- typeMedia[[type]]$href[rowVal[1]]
  # make desired url
  url <- paste0("https://www.fanfiction.net", href)
  returns <- list(url=url,title=title)
  return(returns)
}

#------------------------------------------------------------
# get pages to iterate through in search of title
#------------------------------------------------------------
getPagesTitle <- function(returns) {
  url <- returns[[1]]
  title <- returns[[2]]
  returns <-list()
  library(rvest)
  
  page <- url %>% 
    read_html() %>% 
    html_nodes(xpath='//*[@id="content_wrapper_inner"]/center[1]/a[4]') %>% 
    html_attr("href")
  
  if(length(page)>0){ 
    # if there are multiple pages of entries
    page.g <- strsplit(page, "=", " ")
    page_num <- as.numeric(page.g[[1]][4])
    page_use <- as.character(gsub(page_num,"", page))
    
    returns <- list(noPages=FALSE, page_use=page_use, page_num=page_num, url=url, title=title)
  } else {
    returns <- list(noPages=TRUE, page_use=NA, page_num=NA,url=url, title=title)
  }
  
  return(returns)
}

#------------------------------------------------------------
# Check if page has the story title
#------------------------------------------------------------
getTitles <- function(returns) {
  
  noPages <- returns[[1]]
  page_use <- returns[[2]]
  page_num <- returns[[3]]
  url <- returns[[4]]
  title <- returns[[5]]
  
  foundIt <- 0
  
  scrapeEntries <- function(url, foundIt){
    entryHref<-character(0)
    for(i in 4:28){
      if(foundIt==1) break
      entry <- url %>% read_html() %>% 
        html_nodes(xpath=paste('//*[@id="content_wrapper_inner"]/div[',i,']/a[1]')) %>% 
        html_text()
      if(entry == title) {
        entryHref <- url %>% read_html() %>% 
          html_nodes(xpath=paste('//*[@id="content_wrapper_inner"]/div[',i,']/a[1]')) %>% 
          html_attr("href")
        foundIt <<-1
        return(entryHref)
        
      } else {
        entryHref <- entryHref
      }
    }
  }
  
  if(noPages==TRUE){
    entries <- scrapeEntries(url, foundIt)
    url <- "https://www.fanfiction.net"
    returns <- list(url=url, href=entries)
    return(returns)
    
  } else {
    
    for(i in 1:page_num) {
      if(foundIt == 0) entries <- scrapeEntries(paste0('https://www.fanfiction.net/',page_use,i), foundIt)
    }
    url <- "https://www.fanfiction.net"
    returns <- list(url=url, href=entries)
    return(returns)
  }
}


#------------------------------------------------------------
# scrape all of the text on the first page of the chapter
#------------------------------------------------------------

#------------------------------------------------------------
# actual scraping of the page
#------------------------------------------------------------
getChapter <- function(returns) {
  url <- returns[[1]]
  href <- returns[[2]]
  
  # get all of the chapter text 
  nodes <- url %>% read_html() %>%
    html_nodes("body > #content_parent > #content_wrapper > #content_wrapper_inner > #storytextp > #storytext > p") %>%
    html_text()
  
  return(nodes)
}

#------------------------------------------------------------
# iterating through the available pages
#------------------------------------------------------------
getAllChapters <- function(returns) {
  url <- returns[[1]]
  href <- returns[[2]]
  url <- paste0(url,href)
  chapters <- url %>% read_html() %>%
    html_nodes(xpath='//*[@id="chap_select"]/option') %>%
    html_attr("value")
  
  if(length(chapters)==0) chapters<-1
  
  # get the total number of chapters
  allChapters <- data.frame(text=numeric(0),
                         chapter=character(0))
  
  for(i in 1:length(unique(chapters))) {
    chapters <- data.frame()
    summary <- rm_between(url, str_match(url, "/[0-9]+"), str_match(url, "[A-Za-z]+-[A-Za-z]+"), extract=TRUE)
    url <- gsub(summary, paste0("/",i,"/"), url)
    returns <- list(url=url, href=href)
    print1<-as.data.frame(getChapter(returns))
    print1[2] <- i
    names(print1) <- c("text", "chapter")
    allChapters <- rbind(allChapters,print1)
  }
  
  return(allChapters)
  
}

#------------------------------------------------------------
# FINAL COMPILATION
#------------------------------------------------------------

getFullStory <- function(story, type, title) {
  getAllChapters(getTitles(getPagesTitle(getUrlTitle(story,type,title))))
}