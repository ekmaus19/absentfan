---
title: "Final Project Kaggle"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning = F, error = F)
source("EnhancedROCCurve.R")
load("final.RData")
```
### Team Members  
* Griffin Barich  
* Elizaveta Kravchenko 

## Problem Statement
This project attempts to determine the best way of identifying and classifying "insincere questions" on the Q&A site Quora. The social platform functions through allowing users to both post questions and comment with answers to queries posted by other users. Insincere questions, or posts made without a genuine expectation of a helpful response, challenge the very nature of the platform by including content that can be incendiary or otherwise harmful to those reading, discouraging people from trusting the authenticity of others on the platform. Insincere questions thus inhibit learning from Quora's services by distracting users with toxic, divisive, and incorrect content. For the purposes of this problem, an "insincere question" is classified as one that either uses a "non-neutral tone," is disaparaging or inflammatory, is not grounded in reality, or utilizes sexual or explicit content specifically to shock readers ( [source](https://www.kaggle.com/c/quora-insincere-questions-classification/). This problem is proposed by a Kaggle competition. 

## Data Description
The dataset is a csv file retrieved  from the Kaggle website with 3 variables and approximately 1.3 million rows. The variables include an identification string, the posed question, and code for whether that question is classified as sincere or insincere. Quora provides the dataset and acknowledges that "sanitization measures have been applied" to the data; this data is generated with mild data cleaning and sampling techniques that may make this information not fully representative of question variety on [Quora](https://www.kaggle.com/c/quora-insincere-questions-classification/data). 

Because the models selected for analysis took an exceptionally long time to run on the full data set, only use the first 50,000 entries are used.

More information and full documentation can be found [here](https://www.kaggle.com/c/quora-insincere-questions-classification/data).

## Data Preprocessing

The identification variable is excluded from the dataset, as it is useless to our analysis.

Preprocessing this data is an interesting task, as we only had one string to work with for each observation. To split this one variable into many features we employ the techniques used within the \code{tm} package. The steps were as follows:

1. Create a Corpus from the question variable.

2. Clean the corpus using tm_map function also stemming word variants using \code{SnowballC} package.

3. Coerce the corpus into a DocumentTermMatrix keeping only words that show up 10 or more times.

4. Change all values greater than 1 to 1.

5. Save the Matrix as a data frame for further analysis.

There is no missing data, so no associated methods were implemented.

All random processes in both preprocessing, regression, and analysis are performed with the seed 1234.

```{r PreProcessing, eval=F, include=F}
data <- read.csv("train.csv", stringsAsFactors = F)
library(caret)
load("dtm.rda")
library(tm)
Corpus <- Corpus(VectorSource(data$question_text))
# clean up the corpus using tm_map()
corpus_clean <- tm_map(Corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# stem word variants (e.g. learn, learned, learning)
library(SnowballC)
corpus_clean <- tm_map(corpus_clean, stemDocument)
data_dtm <- DocumentTermMatrix(corpus_clean)
save(data_dtm, file = "dtm.Rda")

data_dtm_exp <- data_dtm[1:50000,]
data_dtm_labels <- factor(data$target[1:50000])

data_dtm_exp2 <- data_dtm[50000:60000,]
data_dtm_labels2 <- factor(data$target[50000:60000])

dtm_freq_words <- findFreqTerms(data_dtm_exp, 10)

data_dtm_freq_exp <- data_dtm_exp[,dtm_freq_words]
data_dtm_freq_exp2 <- data_dtm_exp2[,dtm_freq_words]

# convert counts to a factor
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}

data_exp <- apply(data_dtm_freq_exp, 2, convert_counts)
data_exp2 <- apply(data_dtm_freq_exp2, 2, convert_counts)

save(data_exp, file="nbtrain.rda", compress = "xz")
save(data_dtm_labels, file="nbnames.rda", compress="xz")
save(data_exp2, file="nbtest.rda", compress = "xz")
save(data_dtm_label2s, file="nbnames2.rda", compress="xz")

data_df_exp <- as.data.frame(data_exp)
data_df_exp$LABEL <- data_dtm_labels

save(data_df_exp, file = "data_df.rda", compress = "xz")
set.seed(1234)
index <- createDataPartition(data_df_exp$LABEL, p=0.8, list=F)
train <- data_df_exp[index,]
test <- data_df_exp[-index,]
```


## Machine Learning Approaches

#### Naive Bayes
Bayesian statistics uses Bayes theorem to aggregate the probability of a certain condition (here, whether or not a question is insincere), given probabilities of many features within the condition (here, words). That is, for this problem, the set of unique words in the questions are treated as features and the Bayes Theorem is used to find the probability of an insincere question. Naive Bayes incurs two assumptions; that all features being used are equally important, and that each feature is independent. So, this model must assume that each word in a question is independent from all other words, which might not necessarily be the case. With the aggregate of features and the large amount of observations, however, these assumptions don't hurt the model hugely.

```{r Naive Bayes Work, include=F, eval=F}
exp_classifier <- naiveBayes(data_exp, data_dtm_labels, laplace=1)
save(exp_classifier, file="nb.rda", compress = "xz")
```

#### Logistic Regression

Logisitic Regression is the go-to standard for two-class classification. This approach fits a logistic curve to the data, applying a linear regression approach in a way that is appropriate specifically for a classification problem. The logistic curve is analagous to a best fit line in cases where a numeric outcome is being predicted. It gives each variable a weight based on its overall effect and adds up the effects to make a model that is decent at classification. 

```{r Logistic Regression Work, include=F, eval=F}
logregbasic <- glm(LABEL ~ ., data=train, family = "binomial")
save(logregbasic, file="logireg.rda", compress = "xz")
```

#### Lasso/Ridge Regression

Lasso or Ridge Regression are improvements to logistic regression that specifically manage collinearity (that is, a situation where a predictor/word could be linearly predicted by other predictors with reasonable accuracy). These methods use additional constraints to limit the impact of the total amount of coefficients in the logistic regression, by causing collinear coefficients to shrink to zero. Lasso uses an absolute value of the coefficient, while ridge uses the coefficient squared to "regularize" the model. The additional constraints add bias, but decrease variance, so the model becomes more applicable. We chose to use both lasso and ridge regression on this dataset because the regularization could help improve on the noise of the 1223 variables (words).

```{r Lasso Regression Work, include=F, eval=F}
model.lasso <- glmnet(x, y, family="binomial",nlambda = 20, alpha = 1)
set.seed(1234)
cv.lambda <- cv.glmnet(x, y, family="binomial", nfolds = 5, parallel = T)
save(model.lasso, file="lasso.rda", compress="xz")
save(cv.lambda, file="lasso2.rda", compress="xz")
```

```{r Ridge Regression Work, include=F, eval=F}
model.ridge <- glmnet(x, y, family="binomial",nlambda = 20, alpha = 0)
set.seed(1234)
cv.lambda2 <- cv.glmnet(x, y, family="binomial", nfolds = 5, parallel = T)
save(model.ridge, file="lasso.rda", compress="xz")
save(cv.lambda2, file="lasso2.rda", compress="xz")
```

#### CART

CART or Decision trees are a choice that works really well for easily explainable models. The process of making a classification tree involves looking at each variable and splitting along a variable that makes two groups with maximum homogeneity. Since we have so many variables, any tree we can come up with would only split once. Due to the nature of the data, each split would indicate the presence of a particular word in a selected question. This means that going further down the split of the tree would indicate the presence of the word indicated by the first variable, AND the word indicated by the second variable (and so on for subsequent splits). This was an overall unsuccessful method. 

```{r Tree Work, include=F, eval=F}
tree <- rpart(LABEL ~ ., data = train, method = "class")
save(tree, file="tree.rda", compress="xz")
```

#### Random Forest

Random Forest uses the concepts of CART and adds bootstrapping to generate several slightly different trees. This model is developed by taking a certain number of samples of a specific size, using repeated sampling, then generating decision trees for each sample. Once made, the trees then "vote" on each observation (or, are averaged) and the majority vote of the trees is given as the predicted value. This approach typically performs better than just one tree. In this case, Random Forest does a much better job finding the true positives in the data than both Regularization or Logistic Regression. We also applied the smote presampling algorithm which both upsamples positive outcomes and downsamples negative ones leading to a better model.

## Results

#### Naive Bayes
```{r NBPred, include=F, eval=F}
data_pred_nb <- predict(object=exp_classifier, newdata = data_exp2, type ="raw")
nb_predcutoff <- factor(ifelse(data_pred_nb[,2] > 0.19, 1, 0))
naiveBayesMatrixCO <- confusionMatrix(nb_predcutoff, data_dtm_labels2, positive = "1")
```
```{r NBGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(naiveBayesMatrixCO$table,color = c("red","green"))
myROC(predictions = data_pred_nb[,2], actual = data_dtm_labels2, cutoffs = cutoffs)
```

Our optimal cutpoint was 0.19.

The Accuracy is `r paste0(round(naiveBayesMatrixCO$overall[[1]]*100, 2),"%.")`
The Sensitivity is `r paste0(round(naiveBayesMatrixCO$byClass[[1]]*100, 2),"%.")`
The Specificity is `r paste0(round(naiveBayesMatrixCO$byClass[[2]]*100, 2),"%.")`

#### Logistic Regression
```{r LogRegPred, include=F, eval=F}
data_pred_logreg <- predict(logregbasic, newdata =test, type="response")
logreg_cutoff <- factor(data_pred_logreg>0.28, levels = c(FALSE, TRUE), labels= c(0, 1))
logRegMatrixCO <- confusionMatrix(logreg_cutoff, test$LABEL, positive = "1")
```
```{r LogRegGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(logRegMatrixCO$table,color = c("red","green"))
myROC(predictions = data_pred_logreg, actual = test$LABEL, cutoffs = cutoffs)
```

Our optimal cutpoint was 0.28.

The Accuracy is `r paste0(round(logRegMatrixCO$overall[[1]]*100, 2),"%.")`
The Sensitivity is `r paste0(round(logRegMatrixCO$byClass[[1]]*100, 2),"%.")`
The Specificity is `r paste0(round(logRegMatrixCO$byClass[[2]]*100, 2),"%.")`

#### Lasso Regression
```{r LasRegPred, include=F, eval=F}
x2 <- model.matrix(LABEL~., test)[,-1]
lambda_lasso <- cv.lambda$lambda.min
data_pred_lasso <- predict(model.lasso, newx = x2, type="response", s=lambda_lasso)
lasso_cutoff <- factor(data_pred_lasso>0.29, levels = c(FALSE, TRUE), labels= c(0, 1))
```
```{r LasRegGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(lassoMatrixCO$table,color = c("red","green"))
myROC(predictions = data_pred_lasso, actual = test$LABEL, cutoffs=cutoffs)
```

Our optimal cutpoint was 0.29.

The Accuracy is `r paste0(round(lassoMatrixCO$overall[[1]]*100, 2),"%.")`
The Sensitivity is `r paste0(round(lassoMatrixCO$byClass[[1]]*100, 2),"%.")`
The Specificity is `r paste0(round(lassoMatrixCO$byClass[[2]]*100, 2),"%.")`

#### Ridge Regression
```{r RidRegPred, include=F, eval=F}
x2 <- model.matrix(LABEL~., test)[,-1]
lambda_ridge <- cv.lambda2$lambda.min
data_pred_ridge <- predict(model.ridge, newx = x2, type="response", s=lambda_ridge)
ridge_cutoff <- factor(data_pred_ridge>0.25, levels = c(FALSE, TRUE), labels= c(0, 1))
ridgeMatrixCO <- confusionMatrix(ridge_cutoff, test$LABEL, positive = "1")
```
```{r RidRegGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(ridgeMatrixCO$table,color = c("red","green"))
myROC(predictions = data_pred_ridge, actual = test$LABEL, cutoffs=cutoffs)
```

Our optimal cutpoint was 0.25.

The Accuracy is `r paste0(round(ridgeMatrixCO$overall[[1]]*100, 2),"%.")`
The Sensitivity is `r paste0(round(ridgeMatrixCO$byClass[[1]]*100, 2),"%.")`
The Specificity is `r paste0(round(ridgeMatrixCO$byClass[[2]]*100, 2),"%.")`

#### CART
```{r CARTRegPred, include=F, eval=F}
data_pred_CART <- predict(tree, newdata = test, type = "prob")
CART_cutoff <- factor(data_pred_CART[,2]>0.6, levels = c(FALSE, TRUE), labels= c(0, 1))
CARTMatrixCO <- confusionMatrix(CART_cutoff, test$LABEL, positive = "1")
```
```{r CARTRegGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(CARTMatrixCO$table,color = c("red","green"))
myROC(predictions = data_pred_CART[,2], actual = test$LABEL, cutoffs = cutoffs)
```

Our optimal cutpoint was 0.6.

The Accuracy is `r paste0(round(CARTMatrixCO$overall[[1]]*100, 2),"%.")`
The Sensitivity is `r paste0(round(CARTMatrixCO$byClass[[1]]*100, 2),"%.")`
The Specificity is `r paste0(round(CARTMatrixCO$byClass[[2]]*100, 2),"%.")`

#### Random Forest
```{r rfRegPred, include=F, eval=F}
data_pred_rf <- predict(fit.rfs, newdata = test)
rf_cutoff <- factor(data_pred_rf>0.1, levels = c(FALSE, TRUE), labels= c(0, 1))
rfMatrixCO <- confusionMatrix(pred, test$LABEL, positive = "1")
```
```{r rfmatrixPreprocess, echo=F, eval=T}
rfMatrixCO <- CARTMatrixCO
rfMatrixCO$table[1:4] <- c(8604, 774, 365, 256)
```
```{r rfRegGraphs, fig.height=4, fig.width=4.5}
fourfoldplot(rfMatrixCO$table,color = c("red","green"))
##myROC(predictions = data_pred_CART[,2], actual = test$LABEL, cutoffs = cutoffs)
```


Our Random Forest file was corrupted in transfer between computers and so we were unable to draw a ROC curve or change the cutpoint. The values shown here use a cutpoint of 0.5.

The Accuracy is 88.61%.
The Sensitivity is 41.22%.
The Specificity is 91.74%.


## Discussion

The best model for finding insincere questions is by far the Naive Bayes model. With an overall accuracy of 91.1% and a sensitivity of almost 70%, it substantially surpasses the other modeling approaches in its ability to correctly identify true positives. There is a chance that, if Random Forest had worked as planned, it could rival Naive Bayes; however, this proved to not be the case in earlier tests before introducing cutpoints.

The cutpoints are generated using the ROCR package. Using the package's performance function, it finds the optimal cutpoint based on which metric to maximize. Due to the nature of the problem, a decision is made to maximize sensitivity. This results in rather low cutpoints, as the insincere event is rare in comparison to the negative case.

The best hypothetical application of a machine learning model on the Quora data set would be as a "spam filter" of sorts that correctly identifies insincere questions on the platform. A high sensitivity would contribute to a higher rate of these questions being identified. The model developed through this project offers a few hints as to what terminology could potentially indicate questions needing more scrutiny. Within the Naive Bayes model, the top words most likely to be coded as insincere are "can", "peopl", "like", "get", "will", "trump", "think", "indian", "muslim", "women", "make", "use", "india", "best", and "american." Of specific note are words denoting groups of people ("women", "muslim", "indian"), perhaps highlighting the desire of insincere questions to target groups of people. As noted in the "Problem" section, insincere comments tend to be inflammatory - and the data description on the Kaggle site suggests these comments to be with regards to "a protected class of people," or a "group of people." The presence of words like "trump," "american," and "best," could indicate nationalistic leanings in some of the questions posed on the platform. The term "best" in particular could indicate non-neutrality, also notably present in insincere queries. Words like "can" and "will" typically lead a question (as in, "*can* something be done?", or "*will* this be accomplished?"), and so may not be the most reliable terms to note when looking for specifically incindiary questions.

The model with the best performance after Naive Bayes is the Ridge Regression, with an accuracy value of 93.69%, sensitivity of 47.99%, and specificity of 96.72%. The terms "gayYes", "liberYes", "atheistYes", "sisterYes", "rapeYes", "stupidYes", "peniYes", "muslimYes", "democratYes", "jewYes", "modiYes", "momYes", "trumpYes", "obamaYes", and "hindusYes" are the top coefficients in the Ridge model. The presence of these terms again supports some of the previous allegations. Certain words again ascertain the presence of persecuted groups in discussion, such as "muslimYes", "jewYes", "hindusYes"; in this case, the mention of atheism could contribute to the singling out of religious groups as targets for toxic questioning. As in the first model, political themes again surface in the presence of the coefficients "liberYes," "democratYes," "modiYes" (ie moderate), "trumpYes," and "obamaYes." Due to the charged nature of domestic politics, such themes could co-occur with insincere content. In addition, terminology related to sex also surfaces in this model; the Kaggle competition suggests that mentions of incest and other sexual activity could exist to incite the reader, so the terms "sisterYes", "momYes", "peniYes", "rapeYes", and "gayYes" could all assist in indicating an insincere question.

The CART model shows a high accuracy (93.97%) but an incredibly low sensitivity of 5.31%. As mentioned in the previous discussion of this method, this is likely due to the very nature of this model; because a split on a node would indicate the presence of a particular term, and the presence of another level would compound (ie, would indicate the presence of the second AND the first term), this model proves ineffective in identifying insincere questions. The Random Forest model shows a lower accuracy value (88.61%) as well as a low sensitivity value (41.22%), thereby not being an especially strong current contender for a model with good fit to the data. 

The Naive Bayes model's biggest drawback is that while it correctly predicts insincere questions 70% of the time, it also incorrectly classifies normal questions as insincere 65.2% of the time (false positive rate). One way that this model could be improved would be to run it on a larger subset of the training data set. Due to time constraints, we only used less than 5% of the training dataset given and more questions would lead to more variables in the corpus. 

We wouldn't ask Quora to use this model as it stands right now, but using a Naive Bayes method implemented with Quora's huge dataset would likely lead to a better, more granular prediction engine. 

## References

Kabacoff, Robert. "slides - classification trees." Slides and lecture Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "Feature Selection and Regularization." Slides and lecture Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "Logistic Regression - part 1." Slides and lecture Wesleyan University. Middletown, CT. September 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "Logistic Regression - part 2." Slides and lecture Wesleyan University. Middletown, CT. September 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "Naive Bayes." Slides, lecture and code. Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "slides - regression trees." Slides, code, and code. Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.

Kabacoff, Robert. "slides - Working with missing data." Slides, lecture and code. Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.  

Original source unknown. Retrieved by Kabacoff, Robert. "code - enhanced ROC curve.R." Function. Wesleyan University. Middletown, CT. October 2018. Accessed 12 December, 2018.

“Quora Insincere Questions Classification.” Kaggle, Quora, [Link](www.kaggle.com/c/quora-insincere-questions-classification).
